{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "toc_en"
    ]
   },
   "source": [
    "# INF581 - Advanced Topics in Artificial Intelligence\n",
    "\n",
    "<!--<img src=\"logo.jpg\" style=\"float: left; width: 15%\" />-->\n",
    "\n",
    "[Lab session #04 - Reinforcement Learning I](https://moodle.polytechnique.fr/course/view.php?id=7316)\n",
    "\n",
    "Jérémie DECOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2020/blob/master/lab4solution.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2020/master?filepath=lab4solution.ipynb\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in Binder\" title=\"Open and Execute in Binder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session, we implement classic *Dynamic Programming* algorithms:\n",
    "- Value Iteration\n",
    "- Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment (FrozenLake-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable* thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on FrozenLake-V0: https://github.com/openai/gym/wiki/FrozenLake-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the environment state space and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(range(env.observation_space.n))\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(range(env.action_space.n))\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_labels = {\n",
    "    0: \"Move Left\",\n",
    "    1: \"Move Down\",\n",
    "    2: \"Move Right\",\n",
    "    3: \"Move Up\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_display(state_seq, title=None, figsize=(5,5), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\"):\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "    state_array = np.array(state_seq)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)         # Sample figsize in inches\n",
    "    sns.heatmap(state_array, annot=annot, fmt=fmt, linewidths=linewidths, square=square, cbar=cbar, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_display(state, action):\n",
    "    states_display(transition_array[state,action], title=\"Transition probabilities for action {} ({}) in state {}\".format(action, action_labels[action], state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    actions_src = [\"{}={}\".format(action, action_labels[action].replace(\"Move \", \"\")) for action in actions]\n",
    "    title = \"Policy (\" + \", \".join(actions_src) + \")\"\n",
    "    states_display(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the `is_final_array`, `reward_array` and `transition_array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Dynamic Programming algorithms, you will need the transition probability (or transition function) and the reward function, both defined in `env.P`.\n",
    "\n",
    "Use `env.P[S][A]` to get the list of reachable states from state S executing action A.\n",
    "\n",
    "These reachable states are coded in a tuple defined like this: `(probability, next state, reward, is_final_state)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_final_array = np.full(shape=len(states), fill_value=np.nan, dtype=np.bool)\n",
    "reward_array = np.full(shape=len(states), fill_value=np.NINF)                # np.NINF = negative infinity\n",
    "transition_array = np.zeros(shape=(len(states), len(actions), len(states)))\n",
    "\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        for next_state_tuple in env.P[state][action]:              # env.P[state][action] contains the next states list (a list of tuples)\n",
    "            transition_probability, next_state, next_state_reward, next_state_is_final = next_state_tuple\n",
    "\n",
    "            is_final_array[next_state] = next_state_is_final\n",
    "            reward_array[next_state] = max(reward_array[next_state], next_state_reward)   # workaround: when we already are in state 15, reward is 0 if we stay in state 15 (in practice this never append as the simulation stop when we arrive in state 15 as any other terminal state)\n",
    "            transition_array[state, action, next_state] += transition_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_display(states, fmt=\"d\", title=\"States ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_display(reward_array, title=\"Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_display(is_final_array, fmt=\"d\", title=\"Final states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final states ID\n",
    "is_final_array.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some transitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_display(state=0, action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_display(state=6, action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_display(state=6, action=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the expected value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(state, action, v_array):\n",
    "    return (transition_array[state, action] * v_array).sum() # compute sum(T(s,a,s').V(s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_values(state, v_array):\n",
    "    return (transition_array[state] * v_array).sum(axis=1)   # compute sum(T(s,a,s').V(s')) for all actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute states value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "\n",
    "value_function_history = []\n",
    "delta_history = []\n",
    "\n",
    "def value_iteration(gamma=0.95, epsilon=0.001, display=False):\n",
    "    v_array = np.zeros(len(states))   # Initial value function\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        if display:\n",
    "            states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "        value_function_history.append(v_array)\n",
    "        new_v_array = copy.deepcopy(v_array)\n",
    "        \n",
    "        delta = 0.\n",
    "        \n",
    "        for state in states:\n",
    "            if is_final_array[state]:\n",
    "                new_v_array[state] = reward_array[state]\n",
    "            else:\n",
    "                #assert expected_values(state, v_array).max() == max(  [expected_value(state, action, v_array) for action in actions]  )\n",
    "                #new_v_array[state] = reward_array[state] + gamma * max(  [expected_value(state, action, v_array) for action in actions]  )\n",
    "                new_v_array[state] = reward_array[state] + gamma * expected_values(state, v_array).max()\n",
    "            \n",
    "            delta = max(abs(new_v_array[state] - v_array[state]), delta)\n",
    "        \n",
    "        delta_history.append(delta)\n",
    "        v_array = new_v_array\n",
    "        \n",
    "        if delta < epsilon:\n",
    "            stop = True\n",
    "    \n",
    "    return v_array\n",
    "        \n",
    "v_array = value_iteration(display=True)\n",
    "states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist = pd.DataFrame(value_function_history)\n",
    "df_v_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist.plot()\n",
    "plt.title(\"V(s) w.r.t iteration\")\n",
    "plt.ylabel(\"V(s)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_history)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(r\"$\\max~\\delta$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$\\max~\\delta$\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the (greedy) policy (Maximum Expected Utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, v_array):\n",
    "    return expected_values(state, v_array).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the opimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [greedy_policy(state, v_array) for state in states]\n",
    "display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Value Iteration with Gym (single trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        action = greedy_policy(state, v_array)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        #t += 1\n",
    "\n",
    "    reward_list.append(reward)\n",
    "    #print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
    "\n",
    "print(sum(reward_list) / NUM_EPISODES)            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Value Iteration for different $\\gamma$ with confidence interval (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
    "    v_array = value_iteration(gamma=gamma)\n",
    "    \n",
    "    for episode_index in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = greedy_policy(state, v_array)\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_list.append({\"gamma\": gamma, \"reward\": reward})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reward_list)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean reward (with its 95% confidence interval)\n",
    "\n",
    "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
    "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the VI optimal policy w.r.t. $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
    "    print()\n",
    "    print(\"=\" * 10, \"GAMMA = \", gamma, \"=\" * 10)\n",
    "    print()\n",
    "    \n",
    "    v_array = value_iteration(gamma=gamma)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    policy = [greedy_policy(state, v_array) for state in states]\n",
    "    display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory code: evaluate a given policy (i.e. compute the corresponding value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros(len(states), dtype=int)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.full(shape=(len(states), len(states)), fill_value=np.nan)\n",
    "\n",
    "for state in states:\n",
    "    for next_state in states:\n",
    "        action = policy[state]\n",
    "        A[state, next_state] = -gamma * transition_array[state, action, next_state]\n",
    "        \n",
    "        if state == next_state:\n",
    "            A[state, next_state] = A[state, next_state] + 1.\n",
    "    \n",
    "        #if is_final_array[state]:\n",
    "        #    A[state, next_state] = reward_array[state]   # <- Singular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_display(state=15, action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(A, annot=True, fmt=\"0.2f\", linewidths=.5, cmap=\"Reds\") # , square=True\n",
    "plt.title(r\"Coeficients of the system of Bellman equations for the given policy $\\pi$ and for $\\gamma$=\" + str(gamma));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_array = np.dot(np.linalg.inv(A), reward_array)   # x is the value function\n",
    "states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the (exact) Policy Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma):\n",
    "    A = np.full(shape=(len(states), len(states)), fill_value=np.nan)\n",
    "\n",
    "    for state in states:\n",
    "        for next_state in states:\n",
    "            action = policy[state]\n",
    "            A[state, next_state] = -gamma * transition_array[state, action, next_state]\n",
    "\n",
    "            if state == next_state:\n",
    "                A[state, next_state] = A[state, next_state] + 1.\n",
    "\n",
    "    x = np.dot(np.linalg.inv(A), reward_array)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones(len(states), dtype=int)\n",
    "#policy = np.zeros(len(states), dtype=int)\n",
    "#policy = np.array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype='int') # Optimal policy found by Value Iteration\n",
    "\n",
    "display_policy(policy)\n",
    "\n",
    "v_array = policy_evaluation(policy, gamma)\n",
    "states_display(v_array, title=\"Value function\", cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Policy Improvement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma, initial_policy=None, policy_evaluation_function=policy_evaluation):\n",
    "    # Set the initial policy\n",
    "    if initial_policy is not None:\n",
    "        policy = initial_policy\n",
    "    else:\n",
    "        policy = np.random.randint(low=min(actions), high=max(actions), size=len(states), dtype='int')  # Random initial policy\n",
    "\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        print(policy)\n",
    "\n",
    "        v_array = policy_evaluation_function(policy, gamma)\n",
    "\n",
    "        new_policy = np.copy(policy)\n",
    "        for state in states:\n",
    "            new_policy[state] = greedy_policy(state, v_array)\n",
    "            #greedy_action = greedy_policy(state, v_array)\n",
    "            #if expected_value(state, greedy_action, v_array) > expected_value(state, new_policy[state], v_array):\n",
    "            #    new_policy[state] = greedy_action\n",
    "\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            stop = True\n",
    "        else:\n",
    "            policy = new_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_policy = np.ones(len(states), dtype=int)\n",
    "#initial_policy = np.zeros(len(states), dtype=int)\n",
    "#initial_policy = np.array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype='int') # Optimal policy found by Value Iteration\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "#policy = policy_iteration(gamma=0.99, initial_policy=initial_policy)\n",
    "policy = policy_iteration(gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy Iteration with Gym (single trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #t = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy[state]      # Take a random action\n",
    "        state, reward, done, info = env.step(action)\n",
    "        #t += 1\n",
    "\n",
    "    reward_list.append(reward)\n",
    "    #print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
    "\n",
    "print(sum(reward_list) / NUM_EPISODES)            \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy Iteration for different $\\gamma$ with confidence interval (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
    "    print(\"gamma:\", gamma)\n",
    "    policy = policy_iteration(gamma=gamma)\n",
    "    \n",
    "    for episode_index in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_list.append({\"gamma\": gamma, \"reward\": reward})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reward_list)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean reward (with its 95% confidence interval)\n",
    "\n",
    "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
    "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Policy Iteration (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrice inversion applied in `policy_evaluation` may be prohibtive for environments with big action space.\n",
    "\n",
    "The *Modified Policy Iteration* use a simplified version of Value iteration to ensure the policy evaluation (simplified because there is no `max` operator anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asynchronous_iterative_policy_evaluation(policy, gamma, num_iterations=30, display=False):\n",
    "    \"\"\"Estimate the value function `v_array` using an iterative methode (*modified policy iteration*).\"\"\"\n",
    "\n",
    "    v_array = np.zeros(len(states))\n",
    "    \n",
    "    for iteration_index in range(num_iterations):\n",
    "        \n",
    "        for state in states:\n",
    "            action = policy[state]\n",
    "            v_array[state] = reward_array[state] + gamma * expected_value(state, action, v_array)\n",
    "        \n",
    "        if display:\n",
    "            states_display(v_array, title=\"Value function\", cbar=True)\n",
    "            \n",
    "    return v_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(policy, gamma, num_iterations=30, display=False):\n",
    "    \"\"\"Estimate the value function `v_array` using an iterative methode (*modified policy iteration*).\"\"\"\n",
    "\n",
    "    new_v_array = np.zeros(len(states))\n",
    "    v_array = np.zeros(len(states))\n",
    "    \n",
    "    for iteration_index in range(num_iterations):\n",
    "        \n",
    "        for state in states:\n",
    "            action = policy[state]\n",
    "            new_v_array[state] = reward_array[state] + gamma * expected_value(state, action, v_array)\n",
    "\n",
    "        v_array = new_v_array\n",
    "\n",
    "        if display:\n",
    "            states_display(v_array, title=\"Value function\", cbar=True)\n",
    "    \n",
    "    return v_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Policy Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype='int') # Optimal policy found by Value Iteration\n",
    "\n",
    "display_policy(policy)\n",
    "\n",
    "v_array = policy_evaluation(policy, gamma=gamma)\n",
    "states_display(v_array, title=\"Value function\", cbar=True)\n",
    "\n",
    "v_array = iterative_policy_evaluation(policy, gamma=gamma, display=True)\n",
    "states_display(v_array, title=\"Value function\", cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_policy = np.ones(len(states), dtype=int)\n",
    "#initial_policy = np.zeros(len(states), dtype=int)\n",
    "#initial_policy = np.array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype='int') # Optimal policy found by Value Iteration\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "#policy = policy_iteration(gamma=0.99, initial_policy=initial_policy)\n",
    "policy = policy_iteration(gamma=gamma, policy_evaluation_function=iterative_policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Modified Policy Iteration for different $\\gamma$ with confidence interval (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPISODES = 2000\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
    "    print(\"gamma:\", gamma)\n",
    "    policy = policy_iteration(gamma=gamma, policy_evaluation_function=iterative_policy_evaluation)\n",
    "    \n",
    "    for episode_index in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_list.append({\"gamma\": gamma, \"reward\": reward})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reward_list)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean reward (with its 95% confidence interval)\n",
    "\n",
    "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
    "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
